{
  "hash": "cbcdd29e43ac3a7b03a37c3ec4149478",
  "result": {
    "engine": "knitr",
    "markdown": "---\nfreeze: auto\n---\n\n\n# Arrow {#sec-arrow}\n\n\n\n\n\n## Introdução\n\nArquivos CSV são feitos para serem lidos facilmente por seres humanos.\nEles são um bom formato de intercâmbio porque são muito simples e podem ser lidos por qualquer ferramenta existente.\nPorém, arquivos CSV não são muito eficientes: você tem muito trabalho para importar os dados para o R.\nNeste capítulo, você aprenderá uma alternativa poderosa: o formato [parquet](https://parquet.apache.org/), um formato baseado em um padrão aberto amplamente utilizado em sistemas de grande volume de dados (*big data*).\n\nNós iremos juntar arquivos *parquet* com o [Apache Arrow](https://arrow.apache.org), um conjunto de ferramentas multi-linguagens projetado para análise eficiente e transporte de grandes conjunto de dados.\nUsaremos *Apache Arrow* via o [pacote arrow](https://arrow.apache.org/docs/r/), o qual fornece um *backend* dplyr, permitindo que você analise conjuntos de dados maiores que a quantidade de memória de seu computador, usando a familiar sintaxe dplyr.\nComo benefício adicional, o *arrow* é extremamente rápido: você verá alguns exemplos a seguir neste capítulo.\n\nTanto o pacote *arrow* quanto o pacote dbplyr rodam por trás do dplyr (dplyr *backends*), assim, você deve estar se perguntando quando usar um ou outro.\nEm muitos casos, a escolha já foi feita para você, pois os dados já estão em bancos de dados ou em arquivos *parquet* e você trabalhará com eles da forma como já estão.\nMas se você está começando com seus próprios dados (talvez arquivos CSV), você pode carregá-los em um banco de dados ou convertê-los para *parquet*.\nGeralmente, é dificil saber qual funcionará melhor, então para fases iniciais da tua análise, vamos te encorajar a tentar usar ambos e escolher aquele que funcione melhor para você.\n\n(Um grande obrigado à Danielle Navarro que contribuiu com a versão inicial deste capítulo.)\n\n### Pré-requisitos\n\nNeste capítulo, continuaremos a usar o tidyverse, particularmente o dplyr, mas iremos combiná-lo com o pacote *arrow*, que foi projetado especificamente para trabalhar com grandes conjuntos de dados.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(arrow)\n```\n:::\n\n\nPosteriormente neste capítulo, veremos também algumas conexões entre o *arrow* e o duckdb, portanto também precisaremos do dbplyr e duckdb.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dbplyr, warn.conflicts = FALSE)\nlibrary(duckdb)\n#> Loading required package: DBI\n```\n:::\n\n\n## Obtendo os dados\n\nComeçamos obtendo um conjunto de dados que precise destas ferramentas: um conjunto de dados de items retirados das bibliotecas públicas de *Seattle*, disponível online em [data.seattle.gov/Community/Checkouts-by-Title/tmmm-ytt6](https://data.seattle.gov/Community/Checkouts-by-Title/tmmm-ytt6).\nEste conjunto de dados possui 41.389.465 linhas que informam quantas vezes cada livro foi retirado em cada mês, desde Abril de 2005 até Outubro de 2022.\n\nO código abaixo faz download de uma cópia em *cache* desses dados.\nO dado é um arquivo CSV de 9 GB, portanto leva um certo tempo para baixar.\nEu recomendo fortemente usar `curl::multi_download()` para baixar grandes arquivos já que foi projetado exatamente para este propósito: fornece uma barra de progresso e pode retomar o *download* se for interrompido.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndir.create(\"data\", showWarnings = FALSE)\n\ncurl::multi_download(\n  \"https://r4ds.s3.us-west-2.amazonaws.com/seattle-library-checkouts.csv\",\n  \"data/seattle-library-checkouts.csv\",\n  resume = TRUE\n)\n#> # A tibble: 1 × 10\n#>   success status_code resumefrom url                    destfile        error\n#>   <lgl>         <int>      <dbl> <chr>                  <chr>           <chr>\n#> 1 TRUE            200          0 https://r4ds.s3.us-we… data/seattle-l… <NA> \n#> # ℹ 4 more variables: type <chr>, modified <dttm>, time <dbl>,\n#> #   headers <list>\n```\n:::\n\n\n## Abrindo o conjunto de dados\n\nVamos começar dando uma olhada nos dados.\nCom 9 GB, este arquivo é tão grande que provavelmente não queremos carregá-lo por inteiro na memória do computador.\nComo regra geral, dizemos que você vai querer ter no mínimo duas vezes mais memória que o tamanho dos dados e muitos notebooks chegam até 16 GB.\nIsto significa que queremos evitar `read_csv()` e, ao invés disso, usar `arrow::open_dataset()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv <- open_dataset(\n  sources = \"data/seattle-library-checkouts.csv\", \n  col_types = schema(ISBN = string()),\n  format = \"csv\"\n)\n```\n:::\n\n\nO que acontece quando este código é executado?\n`open_dataset()` irá inspecionar (*scan*) alguns milhares de linhas para entender a estrutura do conjunto de dados.\nA coluna `ISBN` contém valores em branco nas primeiras 80.000 linhas, portanto devemos especificar o tipo da coluna para ajudar o *arrow* a trabalhar com a estrutura de dados.\nUma vez que os dados foram varridos pela `open_dataset()`, ele registra o que encontrou e para; ele lerá mais linhas apenas quando você explicitamente solicitar.\nEste metadado é o que vemos quando imprimimos `seattle_csv`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv\n#> FileSystemDataset with 1 csv file\n#> UsageClass: string\n#> CheckoutType: string\n#> MaterialType: string\n#> CheckoutYear: int64\n#> CheckoutMonth: int64\n#> Checkouts: int64\n#> Title: string\n#> ISBN: string\n#> Creator: string\n#> Subjects: string\n#> Publisher: string\n#> PublicationYear: string\n```\n:::\n\n\nA primeira linha na saída te diz que `seattle_csv` está armazenado localmente em disco como um único arquivo CSV; ele será carregado em memória apenas quando necessário.\nO restante da saída informa o tipo atribuído pelo *arrow* para cada coluna.\n\nPodemos ver o que realmente temos no conjunto de dados com `glimpse()`.\nIsto revela que há \\~41 milhões de linhas e 12 colunas, e nos mostra alguns valores.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv |> glimpse()\n#> FileSystemDataset with 1 csv file\n#> 41,389,465 rows x 12 columns\n#> $ UsageClass      <string> \"Physical\", \"Physical\", \"Digital\", \"Physical\", \"Ph…\n#> $ CheckoutType    <string> \"Horizon\", \"Horizon\", \"OverDrive\", \"Horizon\", \"Hor…\n#> $ MaterialType    <string> \"BOOK\", \"BOOK\", \"EBOOK\", \"BOOK\", \"SOUNDDISC\", \"BOO…\n#> $ CheckoutYear     <int64> 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 20…\n#> $ CheckoutMonth    <int64> 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,…\n#> $ Checkouts        <int64> 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 2, 3, 2, 1, 3, 2,…\n#> $ Title           <string> \"Super rich : a guide to having it all / Russell S…\n#> $ ISBN            <string> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"…\n#> $ Creator         <string> \"Simmons, Russell\", \"Barclay, James, 1965-\", \"Tim …\n#> $ Subjects        <string> \"Self realization, Conduct of life, Attitude Psych…\n#> $ Publisher       <string> \"Gotham Books,\", \"Pyr,\", \"Random House, Inc.\", \"Di…\n#> $ PublicationYear <string> \"c2011.\", \"2010.\", \"2015\", \"2005.\", \"c2004.\", \"c20…\n```\n:::\n\n\nPodemos começar a usar este conjunto de dados com verbos dplyr usando `collect()` para forçar o *arrow* a executar a computação e retornar algum dado.\nPor exemplo, este código nos mostra o número total de retiradas (*checkouts*) por ano (*year*):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv |> \n  group_by(CheckoutYear) |> \n  summarise(Checkouts = sum(Checkouts)) |> \n  arrange(CheckoutYear) |> \n  collect()\n#> # A tibble: 18 × 2\n#>   CheckoutYear Checkouts\n#>          <int>     <int>\n#> 1         2005   3798685\n#> 2         2006   6599318\n#> 3         2007   7126627\n#> 4         2008   8438486\n#> 5         2009   9135167\n#> 6         2010   8608966\n#> # ℹ 12 more rows\n```\n:::\n\n\nGraças ao *arrow*, este código funcionará independente do tamanho do conjunto de dados que tivermos.\nPorém, ainda é um pouco lento: no computador do Hadley demorou \\~10 segundos para concluir.\nNão é tão terrível considerando a quantidade de dados que temos, mas podemos deixá-lo mais rápido mudando para um formato melhor.\n\n## O formato *parquet* {#sec-parquet}\n\nPara tornar este conjunto de dados mais fácil de trabalhar, vamos trocá-lo para o formato de arquivo *parquet* e dividí-lo em múltiplos arquivos.\nAs seções seguintes irão primeiramente introduzir você ao *parquet* e particionamento (*partitioning*) e então aplicar o que aprendemos nos dados das bibliotecas de *Seattle*.\n\n### Vantagens do *parquet*\n\nAssim como CSV, *parquet* é usado para dados retangulares, mas ao invés de ser um formato texto que você pode ler com qualquer editor de arquivo, é um formato binário personalizado projetado especificamente para as necessidades de grandes volumes de dados (*big data*).\nIsto significa que:\n\n-   Arquivos *parquet* são normalmente menores que seus arquivos equivalentes CSV.\n    *Parquet* é baseado em [codificação eficiente](https://parquet.apache.org/docs/file-format/data-pages/encodings/) para manter o tamanho do arquivo menor, e permite a compressão do arquivo.\n    Isto ajuda a tornar os arquivos *parquet* rápidos, pois há menos dados para serem movidos do disco para a memória.\n\n-   Arquivos *parquet* possuem um rico sistema de tipagem.\n    Como falamos na @sec-col-types, um arquivo CSV não fornece nenhuma informação sobre os tipos das colunas.\n    Por exemplo, um leitor de CSV precisa adivinhar se `\"08-10-2022\"` deve ser lido como texto ou data.\n    Diferentemente, arquivos *parquet* armazenam dados de maneira a registrar o tipo juntamente com os dados.\n\n-   Arquivos *parquet* são \"orientados-a-colunas\" (*column-oriented*).\n    Isto significa que eles são organizados coluna por coluna, muito parecido com os *data frames* do R.\n    Isto geramente leva a um melhor desempenho para tarefas de análise de dados quando comparado aos arquivos CSV, que são organizados linha a linha.\n\n-   Arquivos *parquet* são \"fragmentados\" (*chunked*), o que torna possível trabalhar em diferentes partes do mesmo arquivo ao mesmo tempo e, se você tiver sorte, pular alguns fragmentos completamente.\n\nHá uma desvantagem primária nos arquivos *parquet*: eles não podem mais ser \"lidos por humanos\", ou seja, se você olhar para um arquivo *parquet* usando `readr::read_file()`, você verá apenas um monte de caracteres sem sentido.\n\n### Particionamento\n\nConforme os conjuntos de dados vão ficando cada vez maiores, armazenar todos os dados em um único arquivo fica cada vez mais problemático, e geralmente é mais útil dividi-los em vários arquivos.\nQuando esta estrutura é feita de forma inteligente, esta estratégia pode levar a melhoras significativas de desempenho, pois muitas análises precisam apenas de um subconjunto dos arquivos.\n\nNão existem regras absolutas sobre como particionar seu conjunto de dados: os resultados dependerão de seus dados, padrões de acesso e os sistemas que lêem os dados.\nVocê provavelmente precisará fazer alguns experimentos antes de encontrar o particionamento ideal para sua situação.\nComo uma recomendação aproximada, *arrow* sugere que você evite arquivos menores que 20MB e maiores que 2GB e evite particionamentos que produzam mais que 10.000 arquivos.\nVocê deve também tentar particionar por variáveis usadas em filtros; como você verá em breve, isto permite que o *arrow* evite trabalho desnecessário e leia apenas os arquivos relevantes.\n\n### Reescrevendo os dados das bibliotecas de *Seattle*\n\nVamos aplicar estas ideias nos dados das bibliotecas de *Seattle* e ver como se saem na prática.\nVamos particionar os dados por `CheckoutYear` (ano da retirada), já que é provável que algumas análises queiram apenas olhar para dados recentes e particionar por ano gera 18 fragmentos de tamanho razoável.\n\nPara reescrever os dados, definimos o particionamento usando `dplyr::group_by()` e então salvamos as partições em um diretório com `arrow::write_dataset()`.\n`write_dataset()` tem dois argumentos importantes: um diretório onde criaremos os arquivos e o formato que usaremos.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npq_path <- \"data/seattle-library-checkouts\"\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv |>\n  group_by(CheckoutYear) |>\n  write_dataset(path = pq_path, format = \"parquet\")\n```\n:::\n\n\nIsto leva aproximadamente um minuto para executar; como veremos em breve, este investimento de tempo inicial será compensado, pois tornará as operações futuras muito mais rápidas.\n\nVamos ver o que acabamos de produzir:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(\n  arquivos = list.files(pq_path, recursive = TRUE),\n  tamanho_MB = file.size(file.path(pq_path, arquivos)) / 1024^2\n)\n#> # A tibble: 18 × 2\n#>   arquivos                         tamanho_MB\n#>   <chr>                                 <dbl>\n#> 1 CheckoutYear=2005/part-0.parquet       109.\n#> 2 CheckoutYear=2006/part-0.parquet       164.\n#> 3 CheckoutYear=2007/part-0.parquet       178.\n#> 4 CheckoutYear=2008/part-0.parquet       195.\n#> 5 CheckoutYear=2009/part-0.parquet       214.\n#> 6 CheckoutYear=2010/part-0.parquet       222.\n#> # ℹ 12 more rows\n```\n:::\n\n\nO arquivo CSV único de 9GB foi reescrito como 18 arquivos *parquet*.\nOs nomes dos arquivos usam uma convenção de \"auto-descrição\" usada pelo projeto [Apache Hive](https://hive.apache.org).\nPartições no estilo *Hive* nomeiam as pastas com uma convenção \"chave=valor\" e como você pode imaginar, o diretório `CheckoutYear=2005` contém todos os dados onde o ano da retirada (`CheckoutYear`) é 2005.\nCada arquivo tem entre 100 e 300 MB e o tamanho total agora é aproximandamente 4 GB, um pouco mais que a metade do arquivo CSV original.\nIsto é o que esperamos, já que *parquet* é um formato muito mais eficiente.\n\n## Usando dplyr com *arrow*\n\nAgora que criamos estes arquivos *parquet*, precisamos lê-los novamente.\nNós usamos `open_dataset()` novamente, mas dessa vez informando um diretório:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_pq <- open_dataset(pq_path)\n```\n:::\n\n\nAgora podemos escrever nosso *pipeline* dplyr.\nPor exemplo, poderíamos contar o número total de livros retirados em cada mês dos últimos cinco anos:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquery <- seattle_pq |> \n  filter(CheckoutYear >= 2018, MaterialType == \"BOOK\") |>\n  group_by(CheckoutYear, CheckoutMonth) |>\n  summarize(TotalCheckouts = sum(Checkouts)) |>\n  arrange(CheckoutYear, CheckoutMonth)\n```\n:::\n\n\nEscrever código dplyr para *arrow* é conceitualmente similar ao dbplyr, @sec-import-databases: você escreve o código dplyr, que é automaticamente transformado em uma consulta compreensível para a biblioteca C++ do *Apache Arrow* e que será executada quando você chamar `collect()`.\nSe imprimirmos o objeto `query` podemos ver algumas informações sobre o que esperamos que o *Arrow* nos retorne quando a execução ocorrer:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquery\n#> FileSystemDataset (query)\n#> CheckoutYear: int32\n#> CheckoutMonth: int64\n#> TotalCheckouts: int64\n#> \n#> * Grouped by CheckoutYear\n#> * Sorted by CheckoutYear [asc], CheckoutMonth [asc]\n#> See $.data for the source Arrow object\n```\n:::\n\n\nE podemos obter os resultados chamando `collect()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquery |> collect()\n#> # A tibble: 58 × 3\n#> # Groups:   CheckoutYear [5]\n#>   CheckoutYear CheckoutMonth TotalCheckouts\n#>          <int>         <int>          <int>\n#> 1         2018             1         355101\n#> 2         2018             2         309813\n#> 3         2018             3         344487\n#> 4         2018             4         330988\n#> 5         2018             5         318049\n#> 6         2018             6         341825\n#> # ℹ 52 more rows\n```\n:::\n\n\nAssim como dbplyr, *arrow* entende apenas algumas expressões do R, portanto você pode não conseguir escrever exatamente o mesmo código que escreveria normalmente.\nEntretanto, a lista de operações e funções suportadas é bastante extensa e continua aumentando; veja a lista completa de funções atualmente suportadas digitando `?acero`.\n\n### Desempenho {#sec-parquet-fast}\n\nVamos dar uma olhada rápida em como a mudança de CSV para *parquet* impactou o desempenho.\nPrimeiro, vamos medir quanto tempo demora para calcular o número de livros retirados em cada mês de 2021 quando os dados estão armazenados em um único grande arquivo csv:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv |> \n  filter(CheckoutYear == 2021, MaterialType == \"BOOK\") |>\n  group_by(CheckoutMonth) |>\n  summarize(TotalCheckouts = sum(Checkouts)) |>\n  arrange(desc(CheckoutMonth)) |>\n  collect() |> \n  system.time()\n#>    user  system elapsed \n#>  15.691   1.408  14.932\n```\n:::\n\n\nAgora, vamos usar nossa nova versão do conjunto de dados na qual os dados de retiradas das bibliotecas de *Seattle* foram particionadas em 18 arquivos *parquet* menores:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_pq |> \n  filter(CheckoutYear == 2021, MaterialType == \"BOOK\") |>\n  group_by(CheckoutMonth) |>\n  summarize(TotalCheckouts = sum(Checkouts)) |>\n  arrange(desc(CheckoutMonth)) |>\n  collect() |> \n  system.time()\n#>    user  system elapsed \n#>   0.339   0.054   0.081\n```\n:::\n\n\nA melhora de desempenho de \\~100x é atribuída a dois fatores: O particionamento de vários arquivos e o formato de cada arquivo:\n\n-   O particionamento melhora o desempenho pois esta consulta usa `CheckoutYear == 2021` para filtrar os dados, e *arrow* é inteligente o suficiente para reconhecer que precisa ler apenas 1 dos 18 arquivos *parquet*.\n-   O formato *parquet* melhora o desempenho por armazenar dados em formato binário que pode ser carregado mais diretamente para a memória. O formato colunar (*column-wise*) e os metadados ricos fazem com que o *arrow* precise ler apenas as quatro colunas usadas na consulta (`CheckoutYear`, `MaterialType`, `CheckoutMonth` e `Checkouts`).\n\nEsta diferença de desempenho massiva é o motivo pelo qual vale a pena converter grandes arquivos CSV para *parquet*!\n\n### Usando duckdb com *arrow*\n\nTem uma última vantagem de usar *parquet* e *arrow* --- é muito fácil transformar um conjunto de dados *arrow* em um banco de dados DuckDB (@sec-import-databases) chamando `arrow::to_duckdb()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_pq |> \n  to_duckdb() |>\n  filter(CheckoutYear >= 2018, MaterialType == \"BOOK\") |>\n  group_by(CheckoutYear) |>\n  summarize(TotalCheckouts = sum(Checkouts)) |>\n  arrange(desc(CheckoutYear)) |>\n  collect()\n#> Warning: Missing values are always removed in SQL aggregation functions.\n#> Use `na.rm = TRUE` to silence this warning\n#> This warning is displayed once every 8 hours.\n#> # A tibble: 5 × 2\n#>   CheckoutYear TotalCheckouts\n#>          <int>          <dbl>\n#> 1         2022        2431502\n#> 2         2021        2266438\n#> 3         2020        1241999\n#> 4         2019        3931688\n#> 5         2018        3987569\n```\n:::\n\n\nO legal da função `to_duckdb()` é que a transferência não envolve nenhuma cópia de memória e atende aos objetivos do ecossistema *arrow*: permitir transições transparentes de um ambiente computacional para outro.\n\n### Exercícios\n\n1.  Encontre os livros mais populares para cada ano.\n2.  Qual autor tem mais livros no sistema de bibliotecas de *Seattle*?\n3.  Como as retiradas de livros vs livros eletrônicos (*ebooks*) mudaram ao longo dos últimos 10 anos?\n\n## Resumo\n\nNeste capítulo, você experimentou o pacote *arrow*, que fornece um *backend* dplyr para trabalhar com grandes conjuntos de dados em disco.\nEle pode funcionar com arquivos CSV e é muito mais rápido se você converter seus dados para *parquet*.\nParquet é um formato de dados binários projetado especificamente para análise de dados em computadores modernos.\nUm número muito menor de ferramentas podem funcionar com arquivos parquet em comparação com CSV, mas sua estrutura particionada, compactada e colunar torna sua análise muito mais eficiente.\n\nA seguir, você aprenderá sobre sua primeira fonte de dados não retangular, que você manipulará usando ferramentas fornecidas pelo pacote tidyr.\nVamos nos concentrar nos dados provenientes de arquivos JSON, mas os princípios gerais se aplicam a dados com estrutura baseada em árvore (*tree-like*), independentemente de sua origem.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}