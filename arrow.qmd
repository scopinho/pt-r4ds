---
freeze: true
---

# Arrow {#sec-arrow}

```{r}
#| echo: false

source("_common.R")
```

## Introdução

Arquivos CSV são feitos para serem lidos facilmente por seres humanos.
Eles são um bom formato de intercâmbio porque são muito simples e podem ser lidos por qualquer ferramenta existente.
Porém, arquivos CSV não são muito eficientes: você tem muito trabalho para importar os dados para o R.
Next capítulo, você aprenderá uma alternativa poderosa: o formato [parquet](https://parquet.apache.org/), um formato baseado em um padrão aberto amplamente utilizado em sistemas de grande volume de dados (*big data*).

Nós iremos juntar arquivos *parquet* com o [Apache Arrow](https://arrow.apache.org), um conjunto de ferramentas muti-linguagens projetado para análise eficiente e transporte de grandes conjunto de dados.
Usaremos *Apache Arrow* via [pacote arrow](https://arrow.apache.org/docs/r/), o qual fornece um *backend* dplyr permitindo que você analise conjuntos de dados maiores que a quantidade de memória de seu computador usando a familiar sintaxe dplyr.
Como benefício adicional, *arrow* é extremamente rápido: você verá alguns exemplos a seguir neste capítulo.

Tanto *arrow* quanto dbplyr rodam por trás do dplyr (dplyr *backends*), assim, você deve estar se perguntando quando usar um ou outro.
Em muitos casos, a escolha já está feita por você, com os dados já em bancos de dados ou em arquivos *parquet* e tudo que você pretende é trabalhar com eles da forma como já estão.
Mas se você está começando com teus próprios dados (talvez arquivos CSV), você pode carregá-los em um banco de dados ou convertê-los para *parquet*.
Geralmente, é dificil saber qual funcionará melhor, então para fases iniciais da tua análise, vamos te encorajar a tentar usar ambos e escolher aquele que funcione melhor para você.

(Um grande agradecimento à Danielle Navarro que contruiu com a versão inicial deste capítulo.)

### Pré-requisitos

Neste capítulo, continuaremos a usar tydiverse, particularmente dplyr, mas iremos juntá-lo com o pacote *arrow*, que foi projetado especificamente para trabalhar com grandes volumes de dados.

```{r setup}
#| message: false
#| warning: false
library(tidyverse)
library(arrow)
```

Posteriormente neste capítulo, veremos também algumas conexões entre *arrow* e o duckdb, portanto também precisaremos do dbplyr e duckdb.

```{r}
library(dbplyr, warn.conflicts = FALSE)
library(duckdb)
```

## Obtendo os dados

Começamos obtendo um conjunto de dados que precise destas ferramentas: um conjunto de dados de items retirados das bibliotecas públicas de *Seattle*, disponível online em [data.seattle.gov/Community/Checkouts-by-Title/tmmm-ytt6](https://data.seattle.gov/Community/Checkouts-by-Title/tmmm-ytt6).
Este conjunto de dados possui 41.389.465 linhas que informam quantas vezes cada livro foi retirado em cada mês, desde Abril de 2005 até Outubro de 2022.

O código abaixo baixa uma cópia em *cache* desses dados.
O dado é um arquivo CSV de 9 GB, portanto leva um certo tempo para baixar.
Eu recomendo fortemente usar `curl::multi_download()` para baixar grandes arquivos já que foi projetado exatamente para este propósito: fornece uma barra de progresso e pode retomar o *download* se for interrompido.

```{r}
#| eval: !expr "!file.exists('data/seattle-library-checkouts.csv')"
dir.create("data", showWarnings = FALSE)

curl::multi_download(
  "https://r4ds.s3.us-west-2.amazonaws.com/seattle-library-checkouts.csv",
  "data/seattle-library-checkouts.csv",
  resume = TRUE
)
```

## Abrindo o conjunto de dados

Vamos começar dando uma olhada nos dados.
Com 9 GB, este arquivo é grande o suficiente que provavelmente não queremos carregá-lo completamente na memória do computador.
Como regra geral, dizemos que você vai querer ter no mínimo duas vezes mais memória que o tamanho dos dados e muitos *laptops* chegam até 16 GB.
Isto significa que queremos evitar `read_csv()` e ao invés usar `arrow::open_dataset()`:

```{r open-dataset}
seattle_csv <- open_dataset(
  sources = "data/seattle-library-checkouts.csv", 
  col_types = schema(ISBN = string()),
  format = "csv"
)
```

O que acontece quando este código é executado?
`open_dataset()` irá varrer (*scan*) alguns milhares de linhas para entender a estrutura do conjunto de dados.
A coluna `ISBN` contém valores em branco nas primeiras 80.000 linhas, portanto devemos especificar o tipo da coluna para ajudar o *arrow* a trabalhar com a estrutura de dados.
Uma vez que os dados foram varridos pela `open_dataset()`, ele registra o que encontrou e para; ele lerá mais linhas apenas quando você explicitamente solicitar.
Este metadado é o que vemos quando imprimimos `seattle_csv`:

```{r}
seattle_csv
```

A primeira linha na saída te diz que `seattle_csv` está armazenado localmente em disco como um único arquivo CSV; ele será carregado em memória apenas quando necessário.
O restante da saída informa o tipo que o *arrow* atribuiu para cada coluna.

Podemos ver o que realmente temos dentro com `glimpse()`.
Isto revela que existem \~41 milhões de linhas e 12 colunas, e nos mostra alguns valores.

```{r glimpse-data}
#| cache: true
seattle_csv |> glimpse()
```

Podemos começar a usar este conjunto de dados com verbos dplyr usando `collect()` para forçar o *arrow* a executar a computação e retornar algum dado.
Por exemplo, este código nos mostra o número total de retiradas (*checkouts*) por ano (*year*):

```{r}
#| cache: true
seattle_csv |> 
  group_by(CheckoutYear) |> 
  summarise(Checkouts = sum(Checkouts)) |> 
  arrange(CheckoutYear) |> 
  collect()
```

Graças ao *arrow*, este código funcionará independente do tamanho do conjunto de dados que tivermos.
Porém, é ainda um pouco lento: No computador do Hadley demorou \~10 segundos para concluir.
Não é tão terrível considerando a quantidade de dados que temos, mas podemos deixá-lo mais rápido mudando para um formato melhor.

## O formato *parquet* {#sec-parquet}

Para tornar estes dados mais fáceis de trabalhar, vamos trocá-lo para o formato de arquivo *parquet* e dividí-los em múltiplos arquivos.
As seções seguintes irão primeiramente introduzir você ao *parquet* e particionamento (*partitioning*) e então aplicar o que aprendemos nos dados das bibliotecas de *Seattle*.

### Vantagens do *parquet*

Assim como CSV, *parquet* é usado para dados retangulares, mas ao invés de ser um formato texto que você pode ler com qualquer editor de arquivo, é um formato binário personalizado projetado especificamente para as necessidades de grandes volumes de dados (*big data*).
Isto significa que:

-   Arquivos *parquet* são normalmente menores que seus arquivos equivalentes CSV.
    *Parquet* se baseia em [codificação eficiente](https://parquet.apache.org/docs/file-format/data-pages/encodings/) para manter o tamanho do arquivo menor e suporta compressão de arquivo.
    Isto ajuda a tornar os arquivos *parquet* rápidos, pois há menos dado para ser movido do disco para a memória.

-   Arquivos *parquet* possuem um rico sistema de tipagem.
    Como falamos na @sec-col-types, um arquivo CSV não fornece nenhuma informação sobre os tipos das colunas.
    Por exemplo, um leitor de CSV precisa adivinhar se `"08-10-2022"` deve ser lido como texto ou data.
    Diferentemente, arquivos *parquet* armazenam dados de maneira a registrar o tipo juntamente com o dado.

-   Arquivos *parquet* são "orientados-a-colunas" (*column-oriented*).
    Isto significa que eles são organizados em coluna por coluna, muito parecido com os *data frames* do R.
    Isto geramente leva a um melhor desempenho para tarefas de análise de dados quando comparado aos arquivos CSV, que são organizados linha a linha.

-   Arquivos *parquet* são "fragmentados" (*chunked*), o que torna possível trabalhar ao mesmo tempo em diferentes partes do mesmo arquivo e, se você tiver sorte, pular alguns fragmentos completamente.

Há uma desvantagem primária nos arquivos *parquet*: Eles não podem mais ser "lidos por humanos", e.x: se você olhar para um arquivo *parquet* usando `readr::read_file()`, você verá apenas um monte de caracteres sem sentido.

### Particionamento

Conforme os conjuntos de dados vão ficando cada vez maiores, armazenar todos os dados em um único arquivo fica cada vez mais problemático, e geralmente é mais útil dividi-los em vários arquivos.
Quando esta estrutura é feita de forma inteligente, esta estratégia pode levar a melhoras significativas de desempenho, pois muitas análises precisam apenas de um subconjunto dos arquivos.

Não existem regras absolutas sobre como particionar seu conjunto de dados: os resultados dependerão de seus dados, padrões de acesso e os sistemas que lêem os dados.
Você provavelmente precisará fazer alguns experimentos antes de encontrar o particionamento ideal para tua situação.
Como um guia grosseiro, *arrow* sugere que você evite arquivos menores que 20MB e maiores que 2GB e evite particionamentos que produzam mais que 10.000 arquivos.
Você deve também tentar particionar por variáveis usadas em filtros; como você verá em breve, isto permite o *arrow* evitar bastante trabalho e ler apenas os arquivos relevantes.

### Reescrevendo os dados das bibliotecas de *Seattle*

Vamos aplicar estas ideias nos dados das bibliotecas de *Seattle* e ver como se saem na prática.
Iremos particionar os dados por `CheckoutYear` (ano da retirada), já que é provável que algumas análises queiram apenas olhar para dados recentes e particionar por ano gera 18 fragmentos de um tamanho razoável.

Para gravar os dados, definimos o particionamento usando `dplyr::group_by()` e então salvamos as partições em um diretório com `arrow::write_dataset()`.
`write_dataset()` tem dois argumentos importantes: um diretório onde criaremos os arquivos e o formato que usaremos.

```{r}
pq_path <- "data/seattle-library-checkouts"
```

```{r write-dataset}
#| eval: !expr "!file.exists(pq_path)"

seattle_csv |>
  group_by(CheckoutYear) |>
  write_dataset(path = pq_path, format = "parquet")
```

Isto leva aproximadamente um minuto para executar; como veremos em breve, este investimento de tempo inicial será compensado, pois tornará as operações futuras muito mais rápidas.

Vamos ver o que acabamos de produzir:

```{r show-parquet-files}
tibble(
  arquivos = list.files(pq_path, recursive = TRUE),
  tamanho_MB = file.size(file.path(pq_path, files)) / 1024^2
)
```

O arquivo CSV único de 9GB foi reescrito como 18 arquivos *parquet*.
Os nomes dos arquivos usam uma convensão de "auto-descrição" usada pelo projeto [Apache Hive](https://hive.apache.org).
Partições no estilo *Hive* nomeiam as pastas com uma convenção "chave=valor" e como você pode imaginar, o diretório `CheckoutYear=2005` contém todos os dados onde o ano da retirada (`CheckoutYear`) é 2005.
Cada arquivo tem ente 100 e 300 MB e o tamanho total agora é aproximandamente 4 GB, um pouco mais que a metade do arquivo CSV original.
Isto é o que esperamos já que *parquet* é um formato muito mais eficiente.

## Usando dplyr com *arrow*

Agora que criamos estes arquivos *parquet*, precisamos lê-los novamente.
Nós usamos `open_dataset()` novamente, mas dessa vez informando um diretório:

```{r}
seattle_pq <- open_dataset(pq_path)
```

Agora podemos escrever nosso *pipeline* dplyr.
Por exemplo, poderíamos contar o número total de livros retirados em cada mês dos últimos cinco anos:

```{r books-by-year-query}
query <- seattle_pq |> 
  filter(CheckoutYear >= 2018, MaterialType == "BOOK") |>
  group_by(CheckoutYear, CheckoutMonth) |>
  summarize(TotalCheckouts = sum(Checkouts)) |>
  arrange(CheckoutYear, CheckoutMonth)
```

Escrever código para *arrow* é conceitualmente similar ao dbplyr, @sec-import-databases: você escreve o código dplyr, o qual é automaticamente transformado em uma consulta que a biblioteca C++ do *Apache Arrow* entende e que é depois executado quando você chama `collect()`.
Se imprimirmos o objeto `query` podemos ver algumas informações sobre o que esperamos que o *Arrow* nos retorne quando a execução ocorrer:

```{r}
query
```

E podemos obter os resultados chamando `collect()`:

```{r books-by-year}
query |> collect()
```

Assim como dbplyr, *arrow* entende apenas algumas expressões do R, portanto você pode não conseguir escrever exatamente o mesmo código que escreveria normalmente.
Entretanto, a lista de operações e funcções suportadas é bastante extensa e contínua aumentando, veja a lista completa de funções atualmente suportadas digitando `?acero`.

### Desempenho {#sec-parquet-fast}

Vamos dar uma rápida olhada no impacto de desempenho com a mudança de CSV para *parquet*.
Primeiro, vamos medir quanto tempo demora para calcular o número de livros retirados em cada mês de 2021 quando os dados estão armazenados em um único grande arquivo csv:

```{r dataset-performance-csv}
#| cache: true

seattle_csv |> 
  filter(CheckoutYear == 2021, MaterialType == "BOOK") |>
  group_by(CheckoutMonth) |>
  summarize(TotalCheckouts = sum(Checkouts)) |>
  arrange(desc(CheckoutMonth)) |>
  collect() |> 
  system.time()
```

Agora, vamos usar nossa nova versão do conjunto de dados na qual os dados de retiradas das bibliotecas de *Seattle* foram particionadas em 18 arquivos *parquet* menores:

```{r dataset-performance-multiple-parquet}
#| cache: true

seattle_pq |> 
  filter(CheckoutYear == 2021, MaterialType == "BOOK") |>
  group_by(CheckoutMonth) |>
  summarize(TotalCheckouts = sum(Checkouts)) |>
  arrange(desc(CheckoutMonth)) |>
  collect() |> 
  system.time()
```

A melhora de desempenho de \~100x é atribuída a dois fatores: O particionamento de vários arquivos e o formato de cada arquivo:

-   O particionamento melhora o desempenho pois esta consulta usa `CheckoutYear == 2021` para filtrar os dados, e *arrow* é inteligente o suficiente para reconhecer que precisa ler apenas 1 dos 18 arquivos *parquet*.
-   O formato *parquet* melhora o desempenho por armazenar dados em formato binário que pode ser carregado mais diretamente para a memória. O formato colunar (*column-wise*) e os metadados ricos fazem com que o *arrow* precise ler apenas as quatro colunas usadas na consulta (`CheckoutYear`, `MaterialType`, `CheckoutMonth` e `Checkouts`).

Esta diferença de desempenho massiva é o motivo que compensa converter grandes arquivos CSV para *parquet*!

### Usando duckdb com *arrow*

Tem uma última vantagem de usar *parquet* e *arrow* --- é muito fácil transformar um conjunto de dados *arrow* em um banco de dados DuckDB (@sec-import-databases) chamando `arrow::to_duckdb()`:

```{r use-duckdb}
seattle_pq |> 
  to_duckdb() |>
  filter(CheckoutYear >= 2018, MaterialType == "BOOK") |>
  group_by(CheckoutYear) |>
  summarize(TotalCheckouts = sum(Checkouts)) |>
  arrange(desc(CheckoutYear)) |>
  collect()
```

O legal da função `to_duckdb()` é que a transferência não envolve nenhuma cópia de memória e atende aos objetivos do ecossistema *arrow*: permitir transições transparentes de um ambiente computacional para outro.

### Exercícios

1.  Encontre os livros mais populares para cada ano.
2.  Qual autor tem mais livros no sistema de bibliotecas de *Seattle*?
3.  Como as retiradas de livros vs livros eletrônicos (*ebooks*) mudaram ao longo dos últimos 10 anos?

## Resumo

Nest capítulo, você experimentou o pacote *arrow*, que fornece um *backend* dplyr para trabalhar com grandes conjuntos de dados em disco.
Ele pode funcionar com arquivos CSV e é muito mais rápido se você converter seus dados para *parquet*.
Parquet é um formato de dados binários projetado especificamente para análise de dados em computadores modernos.
Muito menos ferramentas podem funcionar com arquivos parquet em comparação com CSV, mas sua estrutura particionada, compactada e colunar torna a análise muito mais eficiente.

A seguir, você aprenderá sobre sua primeira fonte de dados não retangular, que você manipulará usando ferramentas fornecidas pelo pacote tidyr.
Vamos nos concentrar nos dados provenientes de arquivos JSON, mas os princípios gerais se aplicam a dados semelhantes a árvores (*tree-like*), independentemente de sua origem.
