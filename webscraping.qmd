# Raspagem de dados (*Web scraping*) {#sec-scraping}

```{r}
#| echo: false
#| results: asis

source("_common.R")
mensagem_capitulo_sem_traducao()
```

## Introdução

Este capítulo faz a introdução do básico sobre raspagem de dados (*web scraping*) com o pacote [rvest](https://rvest.tidyverse.org).
Raspagem de dados é uma ferramenta muito útil para extração de dados de páginas web.
Alguns websites oferecem uma API, um conjunto de requisições HTTP estruturadas que retornam dados no formato JSON, com o qual você pode lidar usando as técnicas do @sec-rectangling.
Sempre que possível, você deve usar uma API[^webscraping-1], pois geralmente te retornará dados mais confiáveis.
Entretanto, infelizmente, programar com APIs web está fora do escopo deste livro.
Ao invés disso, ensinaremos sobre raspagem de dados, uma técnica que funciona independentemente de o site fornecer uma API ou não.

[^webscraping-1]: Muitas APIs populares já possuem um pacote no CRAN que as encapsulam, então comece sempre fazendo uma pesquisa antes!

Neste capítulo, discutiremos primeiro sobre ética e legalidade da raspagem de dados antes de falar sobre o básico de HTML.
Você aprenderá o básico sobre seletores CSS para localizar elementos específicos em uma página, e como usar funções do rvest para obter dados de textos e atributos de um HTML para o R.
Depois, discutiremos algumas técnicas para descobrir qual seletor CSS você precisa para a página que está fazendo a raspagem de dados e terminaremos falando sobre alguns estudos de caso e uma breve discussão sobre websites dinâmicos.

### Pré-requisitos

Neste capítulo, iremos focar nas ferramentas fornecidas pelo pacote rvest.
O pacote rvest é um membro do tidyverse, mas não faz parte de seus componentes principais, portanto devemos carregá-lo explicitamente.
Iremos também carregar o tidyverse completo, já que é geralmente muito útil para trabalhar com os dados obtidos da raspagem.

```{r}
#| label: setup
#| message: false

library(tidyverse)
library(rvest)
```

## Ética e legalidade da raspagem de dados

Antes de começarmos a discutir o código que você precisará para efetuar a raspagem de dados, precisamos discutir se é ético e lícito realizá-la.
No geral, a situação é complicada em relação a ambos.

A legislação depende muito de onde você vive.
Entretanto, como princípio geral, se um dado é público, impessoal e factual, você provavelmente não terá problemas[^webscraping-2].
Esses três fatores são importantes porque estão ligados aos termos e condições do site, às informações de identificação pessoal e aos direitos autorais, como discutiremos a seguir.

[^webscraping-2]: Obviamente não somos advogados, e este não é um aconselhamento jurídico.
    Mas este é o melhor resumo que podemos dar depois de ler muito sobre esse assunto.

Se os dados não forem públicos, impessoais ou factuais, ou se você estiver coletando os dados especificamente para ganhar dinheiro com eles, será necessário falar com um advogado.
Em qualquer caso, você deve respeitar os recursos do servidor que hospeda as páginas em que você está efetuando a raspagem de dados.
Mais importante ainda, isso significa que se você estiver fazendo raspagem de muitas páginas, certifique-se de esperar um pouco entre cada requisição.
Um jeito fácil é utilizar o pacote [**polite**](https://dmi3kno.github.io/polite/) de Dmytro Perepolkin.
Ele fará uma pausa automatica entre as requisições e armazenará os resultados (*cache*) para que você não precise solicitar a mesma página duas vezes.

### Termos de serviço

Se você olhar atentamente, descobrirá que muitos websites incluem em algum lugar da página um *link* para "termos e condições" ou "termos de serviço", e se você ler a página atentamente, você geralmente descobrirá que o site especificamente proíbe sua raspagem de dados.
Essas páginas tendem a ser uma apropriação jurídica de terras, onde as empresas fazem reivindicações muito amplas.
É educado respeitar estes termos de serviço sempre que possível, mas considere as reivindicações com cautela.

Os tribunais dos Estados Unidos concluíram que simplesmente colocar os termos de serviço no rodapé do website não é suficiente para que você fique vinculado a eles, e.x., [HiQ Labs v. LinkedIn](https://en.wikipedia.org/wiki/HiQ_Labs_v._LinkedIn).
Em geral, para que você seja submetido aos termos de serviços, você deve ter tido uma ação explícita, como criar uma conta ou marcar uma opção.
Isto torna importante saber se um dado é **público** ou não; se você não precisa ter uma conta para acessá-lo, é improvável que você tenha qualquer vínculo com os termos de serviço.
Note que a situação é diferente na Europa, onde os tribunais concluíram que os termos de serviços são aplicáveis mesmo que você não concorde explicitamente com eles.

### Informações de identificação pessoal

Mesmo que o dado seja público, você deve ter extremo cuidado em fazer raspagem de informações pessoais como nomes, endereços de *email*, números telefônicos, datas de nascimento, etc.
A Europa, em particular, tem leis bem restritas sobre coleta e armazenamento destes tipos de dados ([GDPR](https://gdpr-info.eu/)), e independente de onde você viva, é provável que passe por complicações éticas.
Por exemplo, em 2016, um grupo de pesquisadores rasparam dados públicos contendo informações pessoais (e.x., nomes de usuário, idade, genero, localização, etc.) sobre 70.000 pessoas do site de relacionamento OkCupid e eles liberaram publicamente estes dados sem qualquer tentativa de torná-los anônimos (*anonymization*).
Enquanto os pesquisadores acharam que não havia nada de errado com isso, uma vez que os dados já eram públicos, este trabalho foi largamente condenado devido a preocupações éticas sobre a identificação dos usuários cuja informação foi liberada no conjunto de dados.
Se seu trabalho envolve raspagem de dados de informações com identificação pessoal, nós recomendamos fortemente que você leia sobre o estudo do caso OkCupid[^webscraping-3] bem como casos similares de estudos com éticas de pesquisa questionáveis envolvendo a aquisição e liberação de informações com idenficação pessoal.

[^webscraping-3]: Um exemplo de artigo sobre o estudo do OkCupid foi publicado pela Wired, <https://www.wired.com/2016/05/okcupid-study-reveals-perils-big-data-science>.

### Direitos autorais (*copyright*)

Finalmente, você também deve se preocupar com as leis de direitos autorais.
As leis de direitos autorais são complicadas, mas vale a pena dar uma olhada na [lei estadunidense](https://www.law.cornell.edu/uscode/text/17/102) que descreve exatamente o que é protegido: "\[...\] obras originais de autoria fixadas em qualquer meio de expressão tangível, \[...\]".
Em seguida, descreve categorias específicas em que as leis se aplicam, como obras literárias, obras musicais, filmes e muito mais.
Os dados estão notavelmente ausentes da proteção de direitos autorais.
Isso significa que, desde que você limite sua raspagem de dados a fatos, a proteção de direitos autorais não se aplica.
(Porém, observe que a Europa possui um direito separado "[sui generis](https://en.wikipedia.org/wiki/Database_right)" que protege bases de dados (*databases*).)

Como um breve exemplo, nos Estados Unidos, listas de ingredientes e de instruções não estão sujeitas às leis de direitos autorais, portanto estas leis não podem ser usadas para proteger uma receita.
Mas se esta lista de receitas estiver acompanhada de um conteúdo literário substancial, então ela poderá ser protegida.
É por isso que quando você procura por uma receita na internet, ela é acompanhada de tanto conteúdo.

Se você realmente precisa fazer raspagem de dados de conteúdo original, (como texto ou imagem), você ainda pode estar protegido pela [doutrina de uso justo](https://en.wikipedia.org/wiki/Fair_use) (*doctrine of fair use*).
O uso justo (*fair use*) não é uma regra rígida e rápida, mas pesa uma série de fatores.
É mais provável que se aplique caso você esteja coletando dados para pesquisa ou para fins não comerciais e se limite a coletar apenas o que precisa.

## O básico de HTML

Para fazer raspagem de dados em páginas web, você precisa primeiro entender um pouco sobre **HTML**, a linguagem usada para criar páginas web.
HTML é abreviação **H***yper***T***ext* **M***arkup* **L***anguage* e se parece com algo deste tipo:

``` html
<html>
<head>
  <title>Título da Página</title>
</head>
<body>
  <h1 id='primeiro'>Um cabeçalho</h1>
  <p>Algum texto &amp; <b>algum texto em negrito.</b></p>
  <img src='myimg.png' width='100' height='100'>
</body>
```

HTML tem uma estrutura hierárquica formada por **elementos** que consiste em uma marcação (*tag*) de início (e.x., `<tag>`), opcionalmente um **atributo** (*attributes*) (`id='primeiro'`), e uma marcação de fim[^webscraping-4] (como `</tag>`), e **conteúdo** (*contents*) (tudo entre a marcação de início e de fim).

[^webscraping-4]: Em várias marcações (incluindo `<p>` e `<li>`) a marcação de fim não é obrigatória, mas acreditamos ser melhor incluí-la, pois torna a visualização da estrutura HTML mais fácil.

Como `<` e `>` são usados para início e fim das marcações, você não pode escrevê-los diretamente.
Ao invés disso, você deve usar os caracteres de **fuga** (*escapes*) `&gt;` (maior que ou *greater than*) e `&lt;` (menor que ou *less than*) do HTML.
E como estas fugas usam `&`, se você quiser escrever o "&" (E comercial ou *ampersand*) deve usar a fuga `&amp;`.
Há uma grande variedade de caracteres de fuga no HTML, mas você não precisa se preocupar muito com isto, pois o rvest lida automaticamente com elas para você.

A raspagem de dados é possível porque a maioria das páginas que contém o dado que você quer extrair geralmente possuem uma estrutura consistente.

### Elementos

Existem mais de 100 elementos HTML.
Alguns dos mais importantes são:

-   Toda página HTML deve estar entre um elemento `<html>`, que deve ter dois elementos descendentes (*children*): `<head>`, que contém metadados como título da página, e `<body>`, que tem o conteúdo que você vê através do navegador (*browser*).

-   Marcações de **bloco** (*block*) como `<h1>` (cabeçalho 1 ou *heading* 1), `<section>` (seção ou *section*), `<p>` (parágrafo ou *paragraph*), e `<ol>` (lista ordenada ou *ordered list*) formam a estrutura geral da página.

-   Marcações **em linha** (*inline*) como `<b>` (negrito ou *bold*), `<i>` (itálico ou *italics*), e `<a>` (*link*) formatam o texto dentro das marcações de bloco.

Se você encontrar uma marcação que nunca viu antes, você pode pesquisar o que ela faz usando a pesquisa do *Google*.
Um outro ótimo lugar para começar é o [MDN Web Docs](https://developer.mozilla.org/en-US/docs/Web/HTML) que descreve todos os aspectos da programação web.

A maioria dos elementos podem ter conteúdo entre suas marcações de início e fim.
Este conteúdo pode ser um texto ou outros elementos.
Por exemplo, o HTML a seguir contém um parágrafo de texto com uma palavra em negrito.

```         
<p>
  Olá! Meu <b>nome</b> é Hadley.
</p>
```

Os **descendentes** (*children*) são os elementos contidos em outro, portanto, o elemento `<p>` acima possui um descendente, o elemento `<b>`.
O elemento `<b>` não possui descendentes, porém ele tem conteúdo (o texto "nome").

### Atributos

Marcações podem ter **atributos** (*attributes*) com nomes que se parecem com `nome1='valor1' nome2='valor2'`.
Dois dos mais importantes atributos são `id` e `class`, que são usados juntamente com as folhas de estilo CSS (*Cascading Style Sheets*) para controlar a aparência visual da página.
Eles são muito úteis quando raspamos dados de uma página.
Atributos também são usados para gravar os destinos dos *links* (o atributo `href` do elemento `<a>`) e a origem de imagens (o atributo `src` do elemento `<img>`).

## Extraindo dados

Para começar com a raspagem de dados, você precisará do endereço (URL) da página que deseja fazer a raspagem, a qual normalmente pode ser copiada do seu navegador.
Você precisará então importar o HTML daquela página para o R com `read_html()`.
Esta função retorna um objeto `xml_document`[^webscraping-5] que você então irá manipular usando as funções do rvest:

[^webscraping-5]: Esta classe vem do pacote [xml2](https://xml2.r-lib.org).
    xml2 é um pacote de baixo nível a partir do qual o rvest foi criado.

```{r}
html <- read_html("http://rvest.tidyverse.org/")
html
```

rvest também possui uma função que te permite criar um HTML (*inline*).
Usaremos muito isso neste capítulo conforme ensinamos várias funções do rvest com exemplos simples.

```{r}
html <- minimal_html("
  <p>Este é um parágrafo</p>
  <ul>
    <li>Esta é uma lista com marcadores</li>
  </ul>
")
html
```

Agora que você tem o HTML no R, é hora de extrair os dados de interesse.
Você aprenderá primeiro sobre seletores CSS, os quais permitem que você identifique elementos de interesse e sobre as funções do rvest que permitem que você extraia dados desses elementos.
Depois, falaremos brevemente sobre tabelas HTML, que possuem algumas ferramentas especiais.

### Encontrando elementos

CSS é a abreviação para "folha de estilo em cascata" (*cascading style sheets*), que é uma ferramenta para definir os estilos visuais dos documentos HTML.
CSS inclui uma pequena linguagem chamada **seletores CSS** (*CSS Selectors*) para seleção de elementos em uma página.
Seletores CSS definem padrões para localizar elementos HTML e são úteis para raspagem de dados, pois definem uma forma concisa de descrever o elemento do qual você quer extrair os dados.

Retornaremos aos seletores CSS em mais detalhes na @sec-css-selectors, mas felizmente você já pode percorrer um bom caminho com apenas três seletores:

-   `p` seleciona todos elementos `<p>`.

-   `.titulo` seleciona todos elementos com `class` "titulo".

-   `#titulo` seleciona os elemento com o atributo `id` igual a "titulo".
    Atributos Id devem ser únicos dentro de um documento HTML, portanto isto sempre retornará apenas um elemento.

Vamos testar estes seletores com um exemplo simples:

```{r}
html <- minimal_html("
  <h1>Isto é um cabeçalho</h1>
  <p id='primeiro'>Isto é um parágrafo</p>
  <p class='importante'>Isto é um parágrafo importante</p>
")
```

Use `html_elements()` para encontrar todos os elementos que correspondem ao seletor:

```{r}
html |> html_elements("p")
html |> html_elements(".importante")
html |> html_elements("#primeiro")
```

Outra função importante é a `html_element()` que sempre retorna o mesmo número de saídas que entradas.
Se você a usar no documento inteiro, ela retornará a primeira correspondência:

```{r}
html |> html_element("p")
```

Há uma diferença importante entre `html_element()` e `html_elements()` quando você usa um seletor que não corresponde a nenhum elemento.
`html_elements()` retorna um vetor de tamanho 0, enquanto `html_element()` retorna um valor faltante (*missing value*).
Esta diferença será muito importante em breve.

```{r}
html |> html_elements("b")
html |> html_element("b")
```

### Seleções aninhadas (*nesting*)

Na maioria das vezes, você usará `html_elements()` e `html_element()` juntas, geralmente usando `html_elements()` para identificar elementos que virão com várias observações e então usar `html_element()` para identificar elementos que se tornarão variáveis.
Vamos ver isso em ação com um exemplo simples.
Aqui temos uma lista não ordenada (`<ul>`) onde cada item da lista (`<li>`) contém alguma informação sobre quatro personagens de Guerra nas Estrelas (*StarWars*):

```{r}
html <- minimal_html("
  <ul>
    <li><b>C-3PO</b> é um <i>robô</i> que pesa <span class='weight'>167 kg</span></li>
    <li><b>R4-P17</b> é um <i>robô</i></li>
    <li><b>R2-D2</b> é um <i>robô</i> que pesa <span class='weight'>96 kg</span></li>
    <li><b>Yoda</b> pesa <span class='peso'>66 kg</span></li>
  </ul>
  ")
```

Podemos usar `html_elements()` para criar um vetor onde cada elemento corresponde a um personagem diferente:

```{r}
personagens <- html |> html_elements("li")
personagens
```

Para extrair o nome de cada personagem, usamos `html_element()`, pois quando aplicada à saída da `html_elements()` é garantido retornar uma resposta por elemento:

```{r}
personagens |> html_element("b")
```

A diferença entre `html_element()` e `html_elements()` não é importante para o nome, mas é importante para o peso.
Queremos ter um peso para cada personagem, até mesmo quando não há `<span>` peso.
Isto é o que `html_element()` faz:

```{r}
personagens |> html_element(".peso")
```

`html_elements()` encontra todos os `<span>`s peso que são descendentes de `personagens`.
Existem apenas três deles, então perdemos a conexão entre os nomes e os pesos:

```{r}
personagens |> html_elements(".peso")
```

Agora que você selecionou os elementos de interesse, você precisa extrair os dados, sejam do conteúdo texto quanto de alguns atributos.

### Textos e atributos

`html_text2()`[^webscraping-6] extrai o texto puro de um elemento HTML:

[^webscraping-6]: rvest também fornece `html_text()`, porém você deve usar quase sempre `html_text2()`, já que esta faz um trabalho melhor ao converter HTML anihadas em texto.

```{r}
personagens |> 
  html_element("b") |> 
  html_text2()

personagens |> 
  html_element(".peso") |> 
  html_text2()
```

Observe que qualquer caractere de fuga é automaticamente endereçado; você apenas verá estes caracteres no código fonte HTML, mas não nos dados retornados pelo rvest.

`html_attr()` extrai dados dos atributos:

```{r}
html <- minimal_html("
  <p><a href='https://en.wikipedia.org/wiki/Cat'>gatos</a></p>
  <p><a href='https://en.wikipedia.org/wiki/Dog'>cães</a></p>
")

html |> 
  html_elements("p") |> 
  html_element("a") |> 
  html_attr("href")
```

`html_attr()` sempre retorna uma *string*, portanto, se você está extraindo números ou datas, você precisará fazer algum processamento posterior.

### Tabelas

Se você estiver com sorte, seus dados já estarão armazenados em uma tabela HTML, portanto, é apenas uma questão de lê-los diretamenta desta tabela.
Geralmente é muito fácil reconhecer uma tabela em seu navegador: ela terá uma estrutura retangular de linhas e colunas e você pode copiar e colar em uma ferramenta como o Excel.

Tabelas HTML são constituídas por quatro elementos principais: `<table>`, `<tr>` (linha da tabela), `<th>` (cabeçalho da tabela), e `<td>` (dado da tabela).
Aqui está uma tabela HTML simples com duas colunas e três linhas:

```{r}
html <- minimal_html("
  <table class='minha_tabela'>
    <tr><th>x</th>   <th>y</th></tr>
    <tr><td>1.5</td> <td>2.7</td></tr>
    <tr><td>4.9</td> <td>1.3</td></tr>
    <tr><td>7.2</td> <td>8.1</td></tr>
  </table>
  ")
```

rvest fornece uma função que sabe como ler este tipo de dado: `html_table()`.
Ela retorna uma lista contendo um *tibble* para cada tabela encontrada na página.
Use `html_element()` para identificar a tabela que deseja extrair:

```{r}
html |> 
  html_element(".minha_tabela") |> 
  html_table()
```

Note que `x` e `y` foram automaticamente convertidos para números.
Esta conversão automática nem sempre funciona bem, portanto, em cenários mais complexos você deve querer desligá-la com `convert = FALSE` e então fazer sua própria conversão.

## Encontrando os seletores adequados {#sec-css-selectors}

Descobrir o seletor que você precisa para seus dados é geralmente a parte mais difícil do problema.
Você geralmente deverá fazer alguns experimentos para encontrar um seletor que seja ao mesmo tempo específico (e.x. ele não seleciona algo que não interessa) e sensível (e.x. ele seleciona tudo que interessa).
Tentativa e erro é parte normal do processo!
Existem duas principais ferramentas disponíveis para te ajudar com este processo: *SelectorGadget* e as Ferramentas do Desenvolvedor de seu navegador.

[SelectorGadget](https://rvest.tidyverse.org/articles/selectorgadget.html) é um aplicativo (*bookmarklet*) *javascript* que gera seletores automaticamente baseado em exemplos negativos e positivos fornecidos por você.
Ele nem sempre funciona, mas quando o faz, é uma mágica!
Você pode aprender a instalar e usar o *SelectorGadget* lendo <https://rvest.tidyverse.org/articles/selectorgadget.html> ou assistindo o video de Mine em <https://www.youtube.com/watch?v=PetWV5g1Xsc>.

Todo navegador moderno vem com um *kit* de ferramentas para desenvolvedores, mas recomendamos o *Chrome*, mesmo que não seja seu navegador padrão: suas ferramentas para desenvolvedores web são algumas das melhores e estão imediatamente disponíveis.
Clique com o botão direito em um elemento da página e clique `Inspecionar`.
Isto abrirá uma visão expandida da página HTML completa, centralizando o elemento que você acabou de clicar.
Você pode usar isto para explorar a página e ter uma ideia de quais seletores podem funcionar.
Preste atenção aos atributos class e id, uma vez que geralmente são usados para formar a estrutura visual da página, e portanto, são boas ferramentas para extrair os dados que você está procurando.

Dentro do menu Elementos, você também pode clicar com o botão direito em um elemento e selecionar `Copiar como Seletor` para gerar um seletor que identificará de forma única o elemento de interesse.

Caso o *SelectorGadget* ou as Ferramentas do Desenvolvedor (DevTools*) do *Chrome* gerarem um seletor CSS que você não entende, tente [Seletores Explicados](https://kittygiraudel.github.io/selectors-explained/){.uri} (*Selectors Explained*) que traduz seletores CSS para inglês básico.
Caso você se encontre fazendo isso muitas vezes, você pode querer aprender mais sobre seletores CSS em geral.
Recomendamos começar com o engraçado tutorial [jantar CSS](https://flukeout.github.io/) e então conferir o [MDN web docs](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Selectors).

## Juntando tudo

Vamos juntar tudo isso e fazer a raspagem de dados de alguns websites.
Há algum risco destes exemplos não funcionarem mais quando você executá-los --- este é o desafio fundamental da raspagem de dados; se a estrutura do site muda, então você terá que mudar seu código de raspagem.

### Guerra nas Estrelas (*StarWars*)

rvest inclui um simples exemplo na `vignette("starwars")`.
Esta é uma página simples com o mínimo de HTML, portanto, é um bom lugar para se começar.
Eu encorajo você a navegar até essa página agora e usar "Inspecionar Elemento" para inspecionar um dos cabeçalhos que tem o título de um filme de Guerra nas Estrelas.
Use o teclado ou o *mouse* para explorar a hierarquia do HTML e veja se consegue ter uma noção da estrutura compartilhada de cada filme.

Você deve conseguir ver que cada filme possui uma estrutura compartilhada que se parece com isto:

``` html
<section>
  <h2 data-id="1">The Phantom Menace</h2>
  <p>Released: 1999-05-19</p>
  <p>Director: <span class="director">George Lucas</span></p>
  
  <div class="crawl">
    <p>...</p>
    <p>...</p>
    <p>...</p>
  </div>
</section>
```

Nossa meta é transformar isto em um *data frame* com 7 linhas e as variáveis `titulo`, `data_lancamento`, `diretor`, e `introducao`.
Começaremos lendo o HTML e extraindo todos os elementos `<section>`:

```{r}
url <- "https://rvest.tidyverse.org/articles/starwars.html"
html <- read_html(url)

secao <- html |> html_elements("section")
secao
```

Isto retorna sete elementos que correspondem aos sete filmes encontrados na página, sugerindo que usar `section` como seletor é bom.
Extrair cada elemento é direto, já que o dado está sempre presente no texto.
É simplesmente uma questão de encontrar o seletor correto:

```{r}
secao |> html_element("h2") |> html_text2()

secao |> html_element(".director") |> html_text2()
```

Uma vez feito isso para cada componente, podemos encapsular todo o resultado em um *tibble*:

```{r}
tibble(
  titulo = secao |> 
    html_element("h2") |> 
    html_text2(),
  data_lancamento = secao |> 
    html_element("p") |> 
    html_text2() |> 
    str_remove("Released: ") |> 
    parse_date(),
  diretor = secao |> 
    html_element(".director") |> 
    html_text2(),
  introducao = secao |> 
    html_element(".crawl") |> 
    html_text2()
)
```

Nós processamos um pouco mais a `data_lancamento` para obter uma variável que será mais fácil de usar depois em nossas análises.

### Melhores filmes IMDB

Para nossa próxima tarefa, abordaremos algo um pouco mais complicado, extraindo os 250 melhores filmes da base de dados da Internet (IMDb).
Quando este capítulo foi escrito, a página se parecia com a @fig-scraping-imdb.

```{r}
#| label: fig-scraping-imdb
#| echo: false
#| fig-cap: | 
#|   Captura de tela dos melhores filmes da página IMDb feita em 05-12-2022.
#| fig-alt: |
#|   A imagem mostra uma tabela com as colunas "Classificação e Título",
#|   "Nota IMDb" e "Sua Nota". 9 filmes dentre os 250 melhores
#|   são mostrados. Os 5 melhores são "Um sonho de liberdade", "O Poderoso Chefão",
#|   "O Cavaleiro das Trevas", "O Poderoso Chefão: Parte 2" e "Doze Homens e uma Sentença".

knitr::include_graphics("screenshots/scraping-imdb.png", dpi = 300)
```

Este dados têm uma clara estrutura tabular, então vale a pena começar com `html_table()`:

```{r}
url <- "https://web.archive.org/web/20220201012049/https://www.imdb.com/chart/top/"
html <- read_html(url)

tabela <- html |> 
  html_element("table") |> 
  html_table()
tabela
```

Isto inclui algumas colunas vazias, mas no geral, faz um bom trabalho ao capturar as informações da tabela.
No entanto, precisamos fazer mais alguns processamentos para torná-la mais fácil de usar.
Primeiro, renomearemos as colunas para facilitar o trabalho e removeremos os espaços em branco estranhos na classificação (*rank*) e no título (*title*).
Faremos isto com `select()` (ao invés de `rename()`) para renomear e selecionar apenas essas duas colunas em uma única etapa.
Em seguida, removeremos as novas linhas e espaços extras e usaremos `separate_wider_regex()` (da @sec-extract-variables) para extrair o título, ano e classificação em suas próprias variáveis.

```{r}
classificacao <- tabela |>
  select(
    classificacao_titulo_ano = `Rank & Title`,
    nota_imdb = `IMDb Rating`
  ) |> 
  mutate(
    classificacao_titulo_ano = str_replace_all(classificacao_titulo_ano, "\n +", " ")
  ) |> 
  separate_wider_regex(
    classificacao_titulo_ano,
    patterns = c(
      classificacao = "\\d+", "\\. ",
      titulo = ".+", " +\\(",
      ano = "\\d+", "\\)"
    )
  )
classificacao
```

Mesmo neste caso, em que a maioria dos dados vêm de células de tabela, ainda vale a pena dar uma olhada no HTML bruto.
Se você fizer isso, descobrirá que podemos adicionar alguns dados extras usando um dos atributos.
Esse é um dos motivos pelos quais vale a pena gastar um pouco de tempo explorando o código fonte da página; você pode encontrar dados extras ou uma rota de análise um pouco mais fácil.

```{r}
html |> 
  html_elements("td strong") |> 
  head() |> 
  html_attr("title")
```

Podemos combinar isto com os dados tabulares e aplicar novamente `separate_wider_regex()` para extrair os dados que nos interessam:

```{r}
classificacao |>
  mutate(
    classificacao_n = html |> html_elements("td strong") |> html_attr("title")
  ) |> 
  separate_wider_regex(
    classificacao_n,
    patterns = c(
      "[0-9.]+ based on ",
      numero_usuarios = "[0-9,]+",
      " user ratings"
    )
  ) |> 
  mutate(
    numero_usuarios = parse_number(numero_usuarios)
  )
```

## Sites dinâmicos

Até agora nos concentramos em sites onde `html_elements()` retorna o que você vê no navegador e discutimos como processar o que ele retorna e como organizar essas informações em um *data frame*.
Entretanto, algumas vezes você chegará a um site onde `html_elements()` e companhia não retornam nada parecido com o que você vê no navegador.
Em muitos casos, isso ocorre porque você está tentando raspar dados de um site que gera dinamicamente o conteúdo da página com *javascript*.
Atualmente, isso não funciona com o rvest, porque o rvest baixa o HTML bruto e não executa nenhum *javascript*.

Ainda assim é possível raspar os dados desses tipos de sites, mas o rvest precisa usar um processo mais caro: simular totalmente o navegador da web, incluindo a execução de todo *javascript*.
Esta funcionalidade não estava disponível quando escrevemos este livro, mas é algo em que estamos trabalhando ativamente e pode estar disponível quando você ler isto.
Ele usa o [pacote chromote](https://rstudio.github.io/chromote/index.html) que, na verdade, executa um navegador *Chrome* em segundo plano e oferece ferramentas adicionais para interação com o site, como se fosse uma pessoa digitando o texto ou clicando em botões.
Veja maiores informações sobre isto no [website do rvest](http://rvest.tidyverse.org/).

## Resumo

Neste capítulo, você aprendeu sobre o porquê, o porque não e como fazer raspagem de dados em páginas da web.
Primeiro, você aprendeu sobre o básico de HTML e como usar seletores CSS para se referir a elementos específicos, depois aprendeu como usar o pacote rvest para transferir dados do HTML para o R.
Em seguida, demonstramos a raspagem de dados em dois estudos de caso: um cenário mais simples de raspagem de dados do site do pacote rvest com filmes de "Guerra nas Estrelas" e um cenário mais complexo de extração de dados dos 250 melhores filmes do IMDB.

Os detalhes técnicos da raspagem de dados da web podem ser complexos, especialmente quando se trata de sites, mas as considerações legais e éticas podem ser ainda mais complexas.
É importante que você se informe sobre ambos antes de começar a coletar dados.

Isso nos leva ao final da parte de importação do livro, onde você aprendeu técnicas para obter dados de onde eles residem (planilhas, bancos de dados, arquivos JSON e websites) em um formato organizado (*tidy*) para o R.
Agora é hora de voltarmos para um novo tópico: aproveitar ao máximo do R como linguagem de programação.
